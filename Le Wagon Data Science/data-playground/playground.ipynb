{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "HAd6BCnLpoOV"
      },
      "source": [
        "# Fundamentals of Deep Learning -  Playground"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "NBgfxWVkpoOX"
      },
      "source": [
        "üß† Welcome to this module of Deep Learning!\n",
        "\n",
        "üéØ In this challenge, our goal is two-fold:\n",
        "1. Get a visual representation of Neural Networks\n",
        "2. Build a better intuition of what Neural Networks are doing\n",
        "\n",
        "üëâ We will use ***[Tensorflow Playground](https://playground.tensorflow.org/)***\n",
        "\n",
        "_(This first challenge does not require much coding_)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "tYNCnh38poOY"
      },
      "source": [
        "## Classification in Deep Learning"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "hj7-NENtpoOY"
      },
      "source": [
        "### (1) The data"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "gmQa8SYspoOY"
      },
      "source": [
        "‚ùì Let's go to the [Playground](https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=spiral&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=2&seed=0.23545&showTestData=false&discretize=false&percTrainData=70&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false&regularization_hide=true&showTestData_hide=false&stepButton_hide=false&activation_hide=false&problem_hide=false&batchSize_hide=true&dataset_hide=false&resetButton_hide=false&discretize_hide=false&playButton_hide=false&learningRate_hide=true&regularizationRate_hide=true&percTrainData_hide=false&numHiddenLayers_hide=false) and select the following type of data ‚ùì \n",
        "\n",
        "- A classification problem \n",
        "- The circle dataset (<span style=\"color:blue\">blue dots</span> inside a circle of <span style=\"color:orange\">orange dots</span>)\n",
        "- Ratio of training to test data : $ 70 \\% $\n",
        "- No noise ($ = 0$)\n",
        "- Do not show test data (right panel) \n",
        "- Do not discretize the output\n",
        "- Activation function: ***ReLU*** \n",
        "\n",
        "<details>\n",
        "    <summary><i> Why Relu? </i></summary>\n",
        "        \n",
        "üí° In general, try it by default. It appears to work better for many problems!\n",
        "    \n",
        "_Note: Playground only allows you to select **one** activation function that is used for **all** of the **hidden** layers_\n",
        "\n",
        "</details>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "sbs4SwAHpoOZ"
      },
      "source": [
        "### (2) The features"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "B_LGgAzZpoOZ"
      },
      "source": [
        "‚ùì <u>Questions about the features</u> ‚ùì\n",
        "\n",
        "1. Select only the features $X_1$ and $X_2$ (_unselect the other features if necessary_)\n",
        "2. If you were using the other variables such as $X_1^{2}$, $X_2^{2}$, $X_1 X_2$, $sin(X_1)$ and $sin(X_2)$, what type of classic Machine Learning operation does it correspond to?"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "8hVjN4pYpoOZ",
        "tags": [
          "challengify"
        ]
      },
      "source": [
        "#### It corresponds to some type of feature engineering where you transform them.\n",
        "#### Examples: multiplication, sinus, square, ...\n",
        "#### Here, in this exercise but also tomorrow, we will only use the raw input features  X1  and  X2 ."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ncXgKcKXpoOZ"
      },
      "source": [
        "<details>\n",
        "    <summary><i>Answer</i></summary>\n",
        "\n",
        "* It corresponds to some type of ***feature engineering*** where you transform them. \n",
        "    * <i>Examples: multiplication, sinus, square, ...</i>\n",
        "* Here, in this exercise but also tomorrow, we will only use the raw input features $X_1$ and $X_2$. \n",
        "</details>\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "T9WUFA-3poOa"
      },
      "source": [
        "### (3) Building and Fitting a Neural Network in ***Playground***"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "nNTlIwJbpoOa"
      },
      "source": [
        "‚ùì <u>Questions about Neural Networks</u> ‚ùì \n",
        "\n",
        "* üß† Build a model with the following architecture:\n",
        "    - three hidden layers\n",
        "    - 5 neurons on the first hidden layer\n",
        "    - 4 neurons on the second hidden layer\n",
        "    - 3 neurons on the last hidden layer\n",
        "    - In ***Playground***, the output layer is not represented: \n",
        "        - For such binary classification task, keep in mind that it will automatically be a dense layer with 1 neuron activated by the sigmoid function $ \\large \\phi(z) = \\frac{1}{1 + e^{-z}} $\n",
        "\n",
        "* üí™ ***Fit it and stop the iterations when the loss function has stabilized.***\n",
        "\n",
        "* üëÄ Observe carefully:\n",
        "    - Look at the individual neurons and try to understand what each neuron has specialized for during the _.fit()_\n",
        "    - What do you think about the overall shape your results? Re-run the neural network with different activation functions to compare. Can you make it work with \"Linear\"?"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bLGi5mrXpoOa"
      },
      "source": [
        "<details>\n",
        "    <summary>Answer: some insights about the activation functions</summary>\n",
        "\n",
        "- Results may look like a hexagon because ReLu is piece-wise linear!\n",
        "- A non-linearly separable problem cannot be fitted with a linear activation such as **Linear**\n",
        "- Surprisingly, a piece-wise linear activation function such as **ReLu** (or **LeakyReLu**) fits this non-linearly separable problem well (even if that is not always true)\n",
        "- The `tanh` activation gives a \"smoother\" decision boundary\n",
        "- The **sigmoid** does **not** seem to work well here.\n",
        "    \n",
        "üßëüèª‚Äçüè´ Always start with ReLu, it's a safe bet üßëüèª‚Äçüè´!\n",
        "</details>\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6n_zXzEvpoOa"
      },
      "source": [
        "### (4) Building and Fitting a Neural Network in ***Tensorflow.Keras***"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "V2yRx3-dpoOa"
      },
      "source": [
        "üëá We wrote the same model for you - at least the architecture - in Tensorflow's Keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PJ9_8WDkpoOb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-05-22 11:57:16.829052: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-05-22 11:57:17.549034: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
            "2023-05-22 11:57:17.549074: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
            "2023-05-22 11:57:17.623564: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-05-22 11:57:19.201499: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
            "2023-05-22 11:57:19.201782: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
            "2023-05-22 11:57:19.201790: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "2023-05-22 11:57:21.829002: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
            "Your kernel may have been built without NUMA support.\n",
            "2023-05-22 11:57:21.833480: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
            "2023-05-22 11:57:21.833678: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
            "2023-05-22 11:57:21.833804: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
            "2023-05-22 11:57:21.833889: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
            "2023-05-22 11:57:21.833970: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
            "2023-05-22 11:57:21.834152: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
            "2023-05-22 11:57:21.834208: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
            "2023-05-22 11:57:21.834258: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
            "2023-05-22 11:57:21.834267: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
            "Skipping registering GPU devices...\n",
            "2023-05-22 11:57:21.835512: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow as tf\n",
        "\n",
        "model = models.Sequential()\n",
        "\n",
        "model.add(layers.Dense(5, activation='relu', input_dim=2)) # 1st hidden layer with 5 neurons\n",
        "model.add(layers.Dense(4, activation='relu')) # 2nd hidden layer with 4 neurons\n",
        "model.add(layers.Dense(3, activation='relu')) # 3rd hidden layer with 3 neurons\n",
        "\n",
        "\n",
        "\n",
        "model.add(layers.Dense(1, activation='sigmoid')) # Output layer that outputs a probability of belonging\n",
        "                                                 # to the class of \"success\""
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "2hP4Auv8poOb"
      },
      "source": [
        "<details>\n",
        "    <summary><i>What to understand about the code of a Neural Network? </i>üëÜ</summary>\n",
        "\n",
        "- <u>First Hidden Layer a.k.a ***Input Layer***</u>:\n",
        "    - Every datapoint that will be input to the neural network has two features $ X = \\begin{bmatrix} \n",
        "           X_{1} \\\\\n",
        "           X_{2} \\\\\n",
        "         \\end{bmatrix} $.\n",
        "    - You need to inform your Neural Network about the ***number of input features*** through the ***`input_dim` argument***\n",
        "    - A Neural Network tries to mimic the human brain. Here we would like to use 5 neurons to start analyzing each of these points.\n",
        "    \n",
        "    - Every datapoint goes through the first hidden layer which was built using 5 neurons $ layer_1 = \\begin{bmatrix} \n",
        "           a_{1} \\\\\n",
        "           a_{2} \\\\\n",
        "           a_{3} \\\\\n",
        "           a_{4} \\\\\n",
        "           a_{5} \\\\           \n",
        "         \\end{bmatrix} $\n",
        "    \n",
        " - <u>Second Hidden Layer</u>:\n",
        "         \n",
        "    - What if we want to ***make the information flow*** through a second hidden layer with 4 neurons? It is totally possible!\n",
        "    - These 4 neurons $ layer_2 = \\begin{bmatrix} \n",
        "           b_{1} \\\\\n",
        "           b_{2} \\\\\n",
        "           b_{3} \\\\\n",
        "           b_{4} \\\\ \n",
        "         \\end{bmatrix} $ from the second layer will analyze the output from the 5 neurons in the first layer\n",
        "    \n",
        "- <u>Third Hidden Layer</u>:\n",
        "        - What if we want the information to **continue to flow** through a third hidden layer with 3 neurons? Again, totally possible!\n",
        "\n",
        "    - Every neuron's output from the second layer goes through the third hidden layer which was built using 3 neurons $ layer_3 = \\begin{bmatrix} \n",
        "           c_{1} \\\\\n",
        "           c_{2} \\\\\n",
        "           c_{3} \n",
        "         \\end{bmatrix} $\n",
        "         \n",
        "    - These 3 neurons analyze the outputs of the neurons in $ layer_2  $ !\n",
        "\n",
        "- <u>Predictive Layer</u>\n",
        "    - You are dealing with a binary classification task\n",
        "    - We could use two neurons to predict the probability of belonging to class A or class B...\n",
        "    - But one neuron predicting the probability of \"success\" is enough\n",
        "\n",
        "- <u>About activation functions</u>\n",
        "    - Despite its simplicity, the ***ReLU*** has proven to be very effective to add some non-linearity to the layers\n",
        "    - For the predictive layer, the best activation function to use for a classification task is the ***sigmoid*** function. That is something we've already discussed during Decision Science and Machine Learning.\n",
        "\n",
        "- <u>About the Sequential aspect of the Network</u>:\n",
        "    - The fact that you are defining a **Sequential** model has a consequence: each layer is aware of its input size based on the output size of the previous layer!\n",
        "    \n",
        "</details>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Pg_hS1mZpoOc"
      },
      "source": [
        "‚ùì How many parameters are involved in this small Neural Network ‚ùì"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0a-p-ZBypoOc",
        "outputId": "d002b004-43ec-4601-f91a-42e37909dd11",
        "tags": [
          "challengify"
        ]
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_8 (Dense)             (None, 5)                 15        \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 4)                 24        \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 3)                 15        \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 1)                 4         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 58\n",
            "Trainable params: 58\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "# YOUR CODE HERE\n",
        "print(model.summary())"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "5SkFAY0opoOc"
      },
      "source": [
        "<details>\n",
        "    <summary><i>Hint</i></summary>\n",
        "\n",
        "‚úÖ You should have 58 parameters\n",
        "    \n",
        "‚ùå If not, double-check your architecture    \n",
        "</details>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bUhgx4appoOc"
      },
      "source": [
        "### (5) The XOR Dataset"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Ps2rIdkQpoOc"
      },
      "source": [
        "‚ùì <u>Playing with the XOR Dataset</u> ‚ùì \n",
        "\n",
        "* On Playground:\n",
        "    - Change the dataset to the \"XOR - Exclusive Or\".\n",
        "    - Try to design a model with two hidden layers that has a very small **test loss** \n",
        "        - Note: you are free to choose the number of neurons per layer yourself.  \n",
        "        \n",
        "* Coding with Tensorflow/Keras:\n",
        "    - Once you have built your model on Playground, code it down below with the Tensorflow/Keras library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cp6hv2JQpoOc",
        "outputId": "19c1b844-feb5-4d8d-b77f-4d6d7104cea9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 219ms/step - loss: 0.2348 - accuracy: 1.0000\n",
            "Test loss: 0.2348\n",
            "Test accuracy: 1.0000\n"
          ]
        }
      ],
      "source": [
        "# Neural Network that can be well fitted to the XOR Dataset\n",
        "\n",
        "inputs = tf.constant([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=tf.float32)\n",
        "targets = tf.constant([[0], [1], [1], [0]], dtype=tf.float32)\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(4, activation='relu', input_dim=2))  # 1st hidden layer with 4 neurons\n",
        "model.add(layers.Dense(2, activation='relu'))  # 2nd hidden layer with 2 neurons\n",
        "model.add(layers.Dense(1, activation='sigmoid'))  # Output layer\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.fit(inputs, targets, epochs=1000, verbose=0)\n",
        "\n",
        "loss, accuracy = model.evaluate(inputs, targets)\n",
        "print(f\"Test loss: {loss:.4f}\")\n",
        "print(f\"Test accuracy: {accuracy:.4f}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "5Hot4cMzpoOc"
      },
      "source": [
        "### (6) The Spiral Dataset"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "RXc9fEdSpoOc"
      },
      "source": [
        "‚ùì <u>Playing with the Spiral Dataset</u> ‚ùì \n",
        "\n",
        "* On Playground:\n",
        "    - Change the dataset to the \"Spiral\".\n",
        "    - Try to design a model with two hidden layers that has a very small **test loss** \n",
        "        - Note: you are free to choose the number of neurons per layer yourself.  \n",
        "        \n",
        "* Coding with Tensorflow/Keras:\n",
        "    - Once you have built your model on Playground, code it down  below with the Tensorflow/Keras library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OGkuJwrapoOd",
        "outputId": "c87365fd-90a0-4189-98cf-07d0fd4533ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "32/32 [==============================] - 0s 2ms/step - loss: 0.0341 - accuracy: 1.0000\n",
            "Test loss: 0.0341\n",
            "Test accuracy: 1.0000\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "theta = np.sqrt(np.random.rand(1000)) * 2 * np.pi  # Angle\n",
        "r = 5 * theta + np.pi  # Radius\n",
        "x = np.cos(theta) * r\n",
        "y = np.sin(theta) * r\n",
        "inputs = np.column_stack((x, y))\n",
        "targets = np.floor(theta / (2 * np.pi / 2)).astype(int) \n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(4, activation='relu', input_dim=2))\n",
        "model.add(layers.Dense(2, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.fit(inputs, targets, epochs=100, verbose=0)\n",
        "loss, accuracy = model.evaluate(inputs, targets)\n",
        "print(f\"Test loss: {loss:.4f}\")\n",
        "print(f\"Test accuracy: {accuracy:.4f}\")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qgHUpDa4poOd"
      },
      "source": [
        "### (7) How Deep should a Neural Network be ? "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "PDBkA553poOd"
      },
      "source": [
        "üëÄ If you compare the number of parameters needed to fit the Spiral Dataset vs. the XOR dataset, the former requires many more weights....\n",
        "\n",
        "üòÉ Actually, if your models are deep enough, you could potentially fit pretty much any pattern...\n",
        "\n",
        "---\n",
        "\n",
        "<details>\n",
        "    <summary><i>Should I create Very Deep Neural Networks? </i></summary>\n",
        "        \n",
        "<u>Examples:</u>\n",
        "    \n",
        "* Think about a human being. The more this person spends time coding in Python, the better he/she will get better at it!\n",
        "    \n",
        "* Think about a student. The more this person studies, the better he/she will pass exams. But sometimes students can study \"too much\" about a topic and forget about the global picture of a course....\n",
        "    \n",
        "<u>Lessons</u>\n",
        "    \n",
        "üß† For Deep Learning Models, the more layers they have, the more opportunities they will have to learn the patterns in the data...\n",
        "\n",
        "‚ùóÔ∏è The problem is about avoiding **overfitting** ‚ùóÔ∏è\n",
        "    \n",
        "‚ò†Ô∏è Add a good deal of noise and you _may_ see that your model will have learned \"too much\" about this noise. \n",
        "  \n",
        "    \n",
        "üìÜ The next lecture **Deep Learning > Optimizers, Loss, & Fitting** is dedicated to helping you understand which techniques we can use to prevent Deep Learning models from overfitting.\n",
        "\n",
        "</details>\n",
        "\n",
        "---\n",
        "\n",
        "<details>\n",
        "    <summary><i>A picture of overfitting in Playground</i></summary>\n",
        "    \n",
        "<img src='https://wagon-public-datasets.s3.amazonaws.com/data-science-images/DL/playground-overfitting.png' width=700 style='margin:auto'>\n",
        "</details>\n",
        "\n",
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "hgtEhV4ypoOd"
      },
      "source": [
        "## Regression in Deep Learning"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "etYQ1oO_poOd"
      },
      "source": [
        "<u>Let's try to complete a Regression Task using Deep Learning</u>\n",
        "\n",
        "\n",
        "This time, the last layer will no longer look like:  \n",
        "```python\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "```\n",
        "\n",
        "but instead  :\n",
        "```python\n",
        "model.add(layers.Dense(1, activation='linear'))\n",
        "```\n",
        "\n",
        "This means that the output of this network is no longer between $0$ and $1$ (probability) but between $ -\\infty$ and $+ \\infty$."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "J6hiXyNapoOd"
      },
      "source": [
        "‚ùì <u>Playing with the Regression Dataset</u> ‚ùì \n",
        "\n",
        "* On Playground:\n",
        "    - Change the dataset to the \"Regression\".\n",
        "    - Try to design a model that has a very small **test loss** \n",
        "        - Note: you are free to choose both the number of layers and the number of neurons per layer yourself \n",
        "        \n",
        "* Coding with Tensorflow/Keras:\n",
        "    - Once you have built your model on Playground, code it down  below with the Tensorflow/Keras library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Omh9YuukpoOd",
        "outputId": "641a9e7c-157f-4fe5-ab5a-ec93978ad98c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "32/32 [==============================] - 1s 2ms/step - loss: 0.9260\n",
            "Test loss: 0.9260\n"
          ]
        }
      ],
      "source": [
        "# Neural Network that can be well fitted to the Regression Dataset\n",
        "np.random.seed(0)\n",
        "X = np.random.rand(1000, 1)\n",
        "y = 3 * X + np.random.randn(1000, 1)\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(4, activation='relu', input_dim=1))\n",
        "model.add(layers.Dense(2, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='linear'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "model.fit(X, y, epochs=100, verbose=0)\n",
        "\n",
        "loss = model.evaluate(X, y)\n",
        "print(f\"Test loss: {loss:.4f}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-C4qTewypoOe"
      },
      "source": [
        "üèÅ You are now ready to do the same things with Tensorflow's Keras directly!\n",
        "\n",
        "üí™ This was a Warm-Up about Neural Networks / Deep Learning Models... (even if, admittedly, our networks in this challenge were not so \"deep\"). \n",
        "\n",
        "\n",
        "üíæ Don't forget to `git add/commit/push` your notebook...\n",
        "\n",
        "üöÄ ... and move on to the next challenge!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
