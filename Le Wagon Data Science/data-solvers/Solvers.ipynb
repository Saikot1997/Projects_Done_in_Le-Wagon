{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solvers ‚öôÔ∏è"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you will investigate the effects of different `solvers` on `LogisticRegression` models."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëá Run the code below to import the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.47</td>\n",
       "      <td>5.97</td>\n",
       "      <td>7.36</td>\n",
       "      <td>10.17</td>\n",
       "      <td>6.84</td>\n",
       "      <td>9.15</td>\n",
       "      <td>9.78</td>\n",
       "      <td>9.52</td>\n",
       "      <td>10.34</td>\n",
       "      <td>8.80</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.05</td>\n",
       "      <td>8.84</td>\n",
       "      <td>9.76</td>\n",
       "      <td>8.38</td>\n",
       "      <td>10.15</td>\n",
       "      <td>6.91</td>\n",
       "      <td>9.70</td>\n",
       "      <td>9.01</td>\n",
       "      <td>9.23</td>\n",
       "      <td>8.80</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.59</td>\n",
       "      <td>10.71</td>\n",
       "      <td>10.84</td>\n",
       "      <td>10.97</td>\n",
       "      <td>9.03</td>\n",
       "      <td>10.42</td>\n",
       "      <td>11.46</td>\n",
       "      <td>11.25</td>\n",
       "      <td>11.34</td>\n",
       "      <td>9.06</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.00</td>\n",
       "      <td>8.44</td>\n",
       "      <td>8.32</td>\n",
       "      <td>9.65</td>\n",
       "      <td>7.87</td>\n",
       "      <td>10.92</td>\n",
       "      <td>6.97</td>\n",
       "      <td>11.07</td>\n",
       "      <td>10.66</td>\n",
       "      <td>8.89</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12.12</td>\n",
       "      <td>13.44</td>\n",
       "      <td>10.35</td>\n",
       "      <td>9.95</td>\n",
       "      <td>11.09</td>\n",
       "      <td>9.38</td>\n",
       "      <td>10.22</td>\n",
       "      <td>9.04</td>\n",
       "      <td>7.68</td>\n",
       "      <td>11.38</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0           9.47              5.97         7.36           10.17       6.84   \n",
       "1          10.05              8.84         9.76            8.38      10.15   \n",
       "2          10.59             10.71        10.84           10.97       9.03   \n",
       "3          11.00              8.44         8.32            9.65       7.87   \n",
       "4          12.12             13.44        10.35            9.95      11.09   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density  sulphates  alcohol  \\\n",
       "0                 9.15                  9.78     9.52      10.34     8.80   \n",
       "1                 6.91                  9.70     9.01       9.23     8.80   \n",
       "2                10.42                 11.46    11.25      11.34     9.06   \n",
       "3                10.92                  6.97    11.07      10.66     8.89   \n",
       "4                 9.38                 10.22     9.04       7.68    11.38   \n",
       "\n",
       "   quality rating  \n",
       "0               6  \n",
       "1               7  \n",
       "2               4  \n",
       "3               8  \n",
       "4               3  "
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"https://wagon-public-datasets.s3.amazonaws.com/05-Machine-Learning/04-Under-the-Hood/solvers_dataset.csv\")\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The dataset consists of different wines üç∑\n",
    "- The features describe different properties of the wines \n",
    "- The target üéØ is a quality rating given by an expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Target engineering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you are going to transform the ratings into a binary target."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëá How many observations are there for each rating?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "rating_counts = df['quality rating'].value_counts()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Create `y` by transforming the target into a binary classification task where quality ratings below 6 are bad [0], and ratings of 6 and above are good [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality rating</th>\n",
       "      <th>binary target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.47</td>\n",
       "      <td>5.97</td>\n",
       "      <td>7.36</td>\n",
       "      <td>10.17</td>\n",
       "      <td>6.84</td>\n",
       "      <td>9.15</td>\n",
       "      <td>9.78</td>\n",
       "      <td>9.52</td>\n",
       "      <td>10.34</td>\n",
       "      <td>8.80</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.05</td>\n",
       "      <td>8.84</td>\n",
       "      <td>9.76</td>\n",
       "      <td>8.38</td>\n",
       "      <td>10.15</td>\n",
       "      <td>6.91</td>\n",
       "      <td>9.70</td>\n",
       "      <td>9.01</td>\n",
       "      <td>9.23</td>\n",
       "      <td>8.80</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.59</td>\n",
       "      <td>10.71</td>\n",
       "      <td>10.84</td>\n",
       "      <td>10.97</td>\n",
       "      <td>9.03</td>\n",
       "      <td>10.42</td>\n",
       "      <td>11.46</td>\n",
       "      <td>11.25</td>\n",
       "      <td>11.34</td>\n",
       "      <td>9.06</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.00</td>\n",
       "      <td>8.44</td>\n",
       "      <td>8.32</td>\n",
       "      <td>9.65</td>\n",
       "      <td>7.87</td>\n",
       "      <td>10.92</td>\n",
       "      <td>6.97</td>\n",
       "      <td>11.07</td>\n",
       "      <td>10.66</td>\n",
       "      <td>8.89</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12.12</td>\n",
       "      <td>13.44</td>\n",
       "      <td>10.35</td>\n",
       "      <td>9.95</td>\n",
       "      <td>11.09</td>\n",
       "      <td>9.38</td>\n",
       "      <td>10.22</td>\n",
       "      <td>9.04</td>\n",
       "      <td>7.68</td>\n",
       "      <td>11.38</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>6.93</td>\n",
       "      <td>4.49</td>\n",
       "      <td>8.25</td>\n",
       "      <td>8.60</td>\n",
       "      <td>9.41</td>\n",
       "      <td>11.07</td>\n",
       "      <td>8.38</td>\n",
       "      <td>10.89</td>\n",
       "      <td>12.42</td>\n",
       "      <td>8.99</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>10.57</td>\n",
       "      <td>9.56</td>\n",
       "      <td>9.83</td>\n",
       "      <td>8.98</td>\n",
       "      <td>9.77</td>\n",
       "      <td>10.04</td>\n",
       "      <td>10.87</td>\n",
       "      <td>11.28</td>\n",
       "      <td>9.57</td>\n",
       "      <td>8.97</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>10.23</td>\n",
       "      <td>10.98</td>\n",
       "      <td>11.74</td>\n",
       "      <td>11.76</td>\n",
       "      <td>8.87</td>\n",
       "      <td>9.03</td>\n",
       "      <td>9.93</td>\n",
       "      <td>9.86</td>\n",
       "      <td>10.04</td>\n",
       "      <td>8.66</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>7.25</td>\n",
       "      <td>3.95</td>\n",
       "      <td>7.03</td>\n",
       "      <td>8.90</td>\n",
       "      <td>8.49</td>\n",
       "      <td>9.75</td>\n",
       "      <td>11.45</td>\n",
       "      <td>10.21</td>\n",
       "      <td>8.32</td>\n",
       "      <td>9.44</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>13.22</td>\n",
       "      <td>11.59</td>\n",
       "      <td>8.72</td>\n",
       "      <td>9.55</td>\n",
       "      <td>8.56</td>\n",
       "      <td>9.79</td>\n",
       "      <td>11.27</td>\n",
       "      <td>10.97</td>\n",
       "      <td>9.34</td>\n",
       "      <td>9.71</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows √ó 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       fixed acidity  volatile acidity  citric acid  residual sugar  \\\n",
       "0               9.47              5.97         7.36           10.17   \n",
       "1              10.05              8.84         9.76            8.38   \n",
       "2              10.59             10.71        10.84           10.97   \n",
       "3              11.00              8.44         8.32            9.65   \n",
       "4              12.12             13.44        10.35            9.95   \n",
       "...              ...               ...          ...             ...   \n",
       "99995           6.93              4.49         8.25            8.60   \n",
       "99996          10.57              9.56         9.83            8.98   \n",
       "99997          10.23             10.98        11.74           11.76   \n",
       "99998           7.25              3.95         7.03            8.90   \n",
       "99999          13.22             11.59         8.72            9.55   \n",
       "\n",
       "       chlorides  free sulfur dioxide  total sulfur dioxide  density  \\\n",
       "0           6.84                 9.15                  9.78     9.52   \n",
       "1          10.15                 6.91                  9.70     9.01   \n",
       "2           9.03                10.42                 11.46    11.25   \n",
       "3           7.87                10.92                  6.97    11.07   \n",
       "4          11.09                 9.38                 10.22     9.04   \n",
       "...          ...                  ...                   ...      ...   \n",
       "99995       9.41                11.07                  8.38    10.89   \n",
       "99996       9.77                10.04                 10.87    11.28   \n",
       "99997       8.87                 9.03                  9.93     9.86   \n",
       "99998       8.49                 9.75                 11.45    10.21   \n",
       "99999       8.56                 9.79                 11.27    10.97   \n",
       "\n",
       "       sulphates  alcohol  quality rating  binary target  \n",
       "0          10.34     8.80               1              1  \n",
       "1           9.23     8.80               1              1  \n",
       "2          11.34     9.06               0              0  \n",
       "3          10.66     8.89               1              1  \n",
       "4           7.68    11.38               0              0  \n",
       "...          ...      ...             ...            ...  \n",
       "99995      12.42     8.99               1              1  \n",
       "99996       9.57     8.97               1              1  \n",
       "99997      10.04     8.66               0              0  \n",
       "99998       8.32     9.44               1              1  \n",
       "99999       9.34     9.71               0              0  \n",
       "\n",
       "[100000 rows x 12 columns]"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "df['binary target'] = df['quality rating'].apply(lambda x: 1 if x >= 6 else 0)\n",
    "\n",
    "df['quality rating'] = df['binary target']\n",
    "y = df['quality rating']\n",
    "df\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Check the class balance of the new binary target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.5001\n",
       "1    0.4999\n",
       "Name: quality rating, dtype: float64"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "class_balance = df['quality rating'].value_counts(normalize=True)\n",
    "class_balance\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Create your `X` by normalising the features. This will allow for fair comparison of different solvers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X = df.drop(['quality rating',\"binary target\"], axis=1) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-14 {color: black;background-color: white;}#sk-container-id-14 pre{padding: 0;}#sk-container-id-14 div.sk-toggleable {background-color: white;}#sk-container-id-14 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-14 label.sk-toggleable__label-arrow:before {content: \"‚ñ∏\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-14 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-14 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-14 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-14 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-14 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-14 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"‚ñæ\";}#sk-container-id-14 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-14 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-14 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-14 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-14 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-14 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-14 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-14 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-14 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-14 div.sk-item {position: relative;z-index: 1;}#sk-container-id-14 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-14 div.sk-item::before, #sk-container-id-14 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-14 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-14 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-14 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-14 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-14 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-14 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-14 div.sk-label-container {text-align: center;}#sk-container-id-14 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-14 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-14\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MinMaxScaler()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-14\" type=\"checkbox\" checked><label for=\"sk-estimator-id-14\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MinMaxScaler</label><div class=\"sk-toggleable__content\"><pre>MinMaxScaler()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MinMaxScaler()"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "X = scaler.fit_transform(X)\n",
    "scaler"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. LogisticRegression solvers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Logistic Regression models can be optimized using different **solvers**. Make a comparison of the available solvers':\n",
    "- Fit time - which solver is **the fastest**?\n",
    "- Precision - **how different** are their respective precision scores?\n",
    "\n",
    "Available solvers for Logistic Regression are `['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']`\n",
    " \n",
    "For more information on these 5 solvers, check out [this Stack Overflow thread](https://stackoverflow.com/questions/38640109/logistic-regression-python-solvers-defintions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit Times:\n",
      "newton-cg: 0.21616172790527344 seconds\n",
      "lbfgs: 0.15663766860961914 seconds\n",
      "liblinear: 0.18175101280212402 seconds\n",
      "sag: 0.37571001052856445 seconds\n",
      "saga: 0.5852327346801758 seconds\n",
      "\n",
      "Precisions:\n",
      "newton-cg: 0.880012433944669\n",
      "lbfgs: 0.880012433944669\n",
      "liblinear: 0.8806806391367503\n",
      "sag: 0.880012433944669\n",
      "saga: 0.880012433944669\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score\n",
    "import time\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=42)\n",
    "\n",
    "solvers = ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
    "\n",
    "fit_times = {}\n",
    "precisions = {}\n",
    "\n",
    "for solver in solvers:\n",
    "    start_time = time.time()\n",
    "    model = LogisticRegression(solver=solver)\n",
    "    model.fit(X_train, y_train)\n",
    "    fit_time = time.time() - start_time\n",
    "    y_pred = model.predict(X_test)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    \n",
    "    fit_times[solver] = fit_time\n",
    "    precisions[solver] = precision\n",
    "\n",
    "print(\"Fit Times:\")\n",
    "for solver, time_val in fit_times.items():\n",
    "    print(f\"{solver}: {time_val} seconds\")\n",
    "\n",
    "print(\"\\nPrecisions:\")\n",
    "for solver, precision_val in precisions.items():\n",
    "    print(f\"{solver}: {precision_val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR ANSWER\n",
    "fastest_solver = \"lbfgs\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>‚ÑπÔ∏è Click here for our interpretation</summary>\n",
    "\n",
    "All solvers should produce similar precision scores because our cost-function is \"easy\" enough to have a global minimum which is found by all 5 solvers. For very complex cost-functions such as in Deep Learning, different solvers may stopping at different values of the loss function.\n",
    "\n",
    "**The wine dataset**\n",
    "    \n",
    "If you check feature importance with sklearn's <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.inspection.permutation_importance.html\">permutation_importance</a> on the current dataset, you'll see many features result in almost 0 importance. Liblinear solver successively moves only along *one* direction at a time, regularizing the others with L1 regularization (a.k.a, setting their beta to 0), which might provide a good fit for a dataset where many features are not that important in predicting the target.\n",
    "\n",
    "‚ùóÔ∏èThere is a cost to searching for the best solver. Sticking with the default (`lbfgs`) may save the most time overall, sklearn provides you this grid for an idea of which solver to choose to start off with: \n",
    "\n",
    "<img src=\"https://wagon-public-datasets.s3.amazonaws.com/05-Machine-Learning/04-Under-the-Hood/solvers-chart.png\" width=700>\n",
    "\n",
    "\n",
    "\n",
    "</details> "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  üß™ Test your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.10.6, pytest-7.1.3, pluggy-1.0.0 -- /home/saikotdasjoy/.pyenv/versions/lewagon/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/saikotdasjoy/code/Saikot1997/data-solvers/tests\n",
      "plugins: asyncio-0.19.0, anyio-3.6.2\n",
      "asyncio: mode=strict\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 1 item\n",
      "\n",
      "test_solvers.py::TestSolvers::test_fastest_solver \u001b[32mPASSED\u001b[0m\u001b[32m                 [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.01s\u001b[0m\u001b[32m ===============================\u001b[0m\n",
      "\n",
      "\n",
      "üíØ You can commit your code:\n",
      "\n",
      "\u001b[1;32mgit\u001b[39m add tests/solvers.pickle\n",
      "\n",
      "\u001b[32mgit\u001b[39m commit -m \u001b[33m'Completed solvers step'\u001b[39m\n",
      "\n",
      "\u001b[32mgit\u001b[39m push origin master\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult(\n",
    "    'solvers',\n",
    "    fastest_solver=fastest_solver\n",
    ")\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Stochastic Gradient Descent"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression models can also be optimized via Stochastic Gradient Descent.\n",
    "\n",
    "‚ùì Evaluate a Logistic Regression model optimized via **Stochastic Gradient Descent**. How do its precision score and training time compare to the performance of the models trained in section 2?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<details>\n",
    "<summary>üí° Hint</summary>\n",
    "\n",
    "- If you are stuck, look at the [SGDClassifier doc](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html)!\n",
    "\n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDClassifier Results:\n",
      "Fit Time: 0.15158867835998535 seconds\n",
      "Precision: 0.8854833630733704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saikotdasjoy/.pyenv/versions/lewagon/lib/python3.10/site-packages/sklearn/linear_model/_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "# Create and fit the SGDClassifier model\n",
    "model = SGDClassifier(loss='log', random_state=42)\n",
    "start_time = time.time()\n",
    "model.fit(X_train, y_train)\n",
    "fit_time = time.time() - start_time\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "\n",
    "print(\"SGDClassifier Results:\")\n",
    "print(\"Fit Time:\", fit_time, \"seconds\")\n",
    "print(\"Precision:\", precision)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚òùÔ∏è The SGD model should have one of the shortest times (maybe even shorter than `liblinear`), for similar performance. This is a direct effect of performing each epoch of the Gradient Descent on a single row as opposed to loading 100k rows into memory at a time."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Predictions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Use the best model (balanced with short fit time and high precision) to predict the binary quality (0 or 1) of the following wine. Store your:\n",
    "- `predicted_class`\n",
    "- `predicted_proba_of_class` (i.e if your model predicted a class of 1 what is the probability it believes 1 to be the class should be between 0 and 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.54</td>\n",
       "      <td>13.5</td>\n",
       "      <td>12.35</td>\n",
       "      <td>8.78</td>\n",
       "      <td>14.72</td>\n",
       "      <td>9.06</td>\n",
       "      <td>9.67</td>\n",
       "      <td>10.15</td>\n",
       "      <td>11.17</td>\n",
       "      <td>12.17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0           9.54              13.5        12.35            8.78      14.72   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density  sulphates  alcohol  \n",
       "0                 9.06                  9.67    10.15      11.17    12.17  "
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_wine = pd.read_csv('https://wagon-public-datasets.s3.amazonaws.com/05-Machine-Learning/04-Under-the-Hood/solvers_new_wine.csv')\n",
    "new_wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9680835386510153"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "best_model = LogisticRegression(solver='lbfgs')\n",
    "best_model.fit(X_train, y_train)\n",
    "new_wine_scaled = scaler.transform(new_wine)\n",
    "predicted_class = best_model.predict(new_wine_scaled)\n",
    "predicted_proba_of_class = best_model.predict_proba(new_wine_scaled)[0][0]\n",
    "predicted_proba_of_class"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üèÅ  Check your code and push your notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.10.6, pytest-7.1.3, pluggy-1.0.0 -- /home/saikotdasjoy/.pyenv/versions/lewagon/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/saikotdasjoy/code/Saikot1997/data-solvers/tests\n",
      "plugins: asyncio-0.19.0, anyio-3.6.2\n",
      "asyncio: mode=strict\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 2 items\n",
      "\n",
      "test_new_data_prediction.py::TestNewDataPrediction::test_predicted_class \u001b[32mPASSED\u001b[0m\u001b[32m [ 50%]\u001b[0m\n",
      "test_new_data_prediction.py::TestNewDataPrediction::test_predicted_proba \u001b[32mPASSED\u001b[0m\u001b[32m [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 0.12s\u001b[0m\u001b[32m ===============================\u001b[0m\n",
      "\n",
      "\n",
      "üíØ You can commit your code:\n",
      "\n",
      "\u001b[1;32mgit\u001b[39m add tests/new_data_prediction.pickle\n",
      "\n",
      "\u001b[32mgit\u001b[39m commit -m \u001b[33m'Completed new_data_prediction step'\u001b[39m\n",
      "\n",
      "\u001b[32mgit\u001b[39m push origin master\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult(\n",
    "    'new_data_prediction',\n",
    "    predicted_class=predicted_class,\n",
    "    predicted_proba_of_class=predicted_proba_of_class\n",
    ")\n",
    "result.write()\n",
    "print(result.check())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
