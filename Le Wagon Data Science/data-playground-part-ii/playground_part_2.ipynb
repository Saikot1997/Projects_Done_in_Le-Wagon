{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playground - Part II"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üéØ **Goal**: Get a better understanding of ***Neural Network hyperparameters***\n",
    "\n",
    "<hr>\n",
    "\n",
    "üëâ Open the [Playground](https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=3&seed=0.06711&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false&regularization_hide=false&regularizationRate_hide=false) again to learn more about Neural Networks. \n",
    "\n",
    "‚ùóÔ∏è Keep in mind that as the algorithm is stochastic, the results may differ from one run to another. For this reason, do not hesitate to re-run the algorithms multiple times to analyse the behavior of your Neural Networks and draw your conclusions accordingly.\n",
    "\n",
    "üïµüèª Let's explore the different items we have seen during the lecture:\n",
    "- **Batch Size**\n",
    "- **Regularization**\n",
    "- **Learning Rate**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) The batch size"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Initial Question** ‚ùì Select the `circle dataset` (Classification). \n",
    "\n",
    "* Build a model with: \n",
    "    * one hidden layer with 3 neurons,\n",
    "    * a _learning rate_ equal to 0.03, \n",
    "    * and the _tanh_ activation function\n",
    "\n",
    "* Do not add any noise (=0).\n",
    "\n",
    "* Select a batch size of 30\n",
    "\n",
    "***Look at the convergence of the algorithm. Does it seem slow or fast?***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <i>untill 1000 epoche it was quite fast , then it was slower</i>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: What is happening with a batch size of 1** ‚ùì \n",
    "\n",
    "Now, run this neural network on the same dataset but... \n",
    "\n",
    "* with a batch-size of 1.\n",
    "* Make sure to run it for at least 150 epochs. \n",
    "\n",
    "***What do you notice about the train and test loss? What is the reason of this instability?***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <i>changes was so fats , test loss stopped at 0.088 , and training loss stopped at 0.016</i>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question/Observation** ‚ùì \n",
    "\n",
    "Now, you can see the effect of the _batch_size_ by reading the values of the train loss and test loss: pause the iterations and run it step by step (iteration per iteration) using the `\"Step\"` button (at the right side of the play/stop button)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <i>itz noisy , but overall decreases over time</i>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Regularization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question about the lack of generalization** ‚ùì \n",
    "\n",
    "To once again observe the **lack of generalization**:\n",
    "* Select the `\"eXclusive OR\"(XOR)` dataset, \n",
    "* with a noise of 50,\n",
    "* Add a second hidden layer with again 8 neurons. \n",
    "\n",
    "***Try to fit your model once again... what do you expect?***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <i>after 300 epoche , changes was slower , test loss was 0.321 and training loss was 0.259</i>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùóÔ∏è With a smaller batch size, your model will end up overfitting faster... ‚ùóÔ∏è\n",
    "\n",
    "üëâ Although, let's keep ***`batch size = 1`*** for the next question and try to understand how to prevent overfitting using the strategy of `regularization`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question about regularization** ‚ùì \n",
    "\n",
    "Can we ***regularize*** our network to ***avoid overfitting***? \n",
    "\n",
    "* Keep the batch size to 1,\n",
    "* Add a `L2-regularization`,\n",
    "* Increase the power of this L2-regularization until it smooths out the decision boundary! \n",
    "Notice how the test loss doesn't increase anymore with the epochs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <i>the more power of L2 regularization , the more train loss and test loss and it is more faster . but regularization rate 0.001 was good enough for decision boundary and considering the train and test loss</i>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Questions with the spiral dataset** ‚ùì \n",
    "\n",
    "<u>Configuration</u>:\n",
    "\n",
    "* Select the `spiral dataset`,\n",
    "* Remove regularization,\n",
    "* Increase the `ratio of training to test data` to 80%. \n",
    "\n",
    "<u>Neural Network</u>: 3 hidden layers with:\n",
    "* 8 neurons on the first layer, \n",
    "* 7 neurons on the second layer,\n",
    "* 6 neurons on the third layer. \n",
    "\n",
    "<u>Experiment</u>:\n",
    "\n",
    "* Run the algorithm with a batch size of 30,\n",
    "* Make sure to run it for at least 1500 epochs,\n",
    "* Then, compare it to the same run but with a batch size of 1. \n",
    "\n",
    "You can check what happens on the train loss and test loss step by step."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <i>too much overfitting with batch 1 , and the decision boundary in not accaeptable range . but for batch 50 , decision boundary is in our acceaptable range , and it was not overfitting , test loss and training loss in far less than with batch 1</i>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) The learning rate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go back to the <u>`circle dataset`</u>:\n",
    "* with no noise,\n",
    "* and a *ratio of training to test data* of 50%,\n",
    "* Use a batch size of 20. \n",
    "\n",
    "Use a <u>neural network</u> with:\n",
    "* one layer of 5 neurons,\n",
    "* no regularization, \n",
    "* and the tanh activation function\n",
    "\n",
    "‚ùì **Question about the learning rate** ‚ùì \n",
    "\n",
    "For each learning rate (from 0.0001 to 10), run the algorithm during 1000 epochs and report the values of the test loss in the list below. Then, plot the test loss with respect to the learning rates. \n",
    "\n",
    "‚ùóÔ∏è <u>Warning</u> ‚ùóÔ∏è When you change the learning rate, make sure to reinitialize the neural network (_circular arrow, left to the play/pause button_)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_circles\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "X, y = make_circles(n_samples=1000, noise=0, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n",
    "\n",
    "\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(5, input_dim=2, activation='tanh'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "batch_size = 20\n",
    "epochs = 1000\n",
    "\n",
    "learning_rates = [0.0001, 0.001, 0.01, 0.1, 1, 10]\n",
    "test_loss = []\n",
    "\n",
    "for lr in learning_rates:\n",
    "    model = create_model()\n",
    "    optimizer = Adam(learning_rate=lr)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=0)\n",
    "    loss = model.evaluate(X_test, y_test, verbose=0)\n",
    "    test_loss.append(loss)\n",
    "\n",
    "plt.plot(learning_rates, test_loss)\n",
    "plt.xlabel('Learning Rates')\n",
    "plt.ylabel('Test Loss')\n",
    "plt.title('Test Loss vs. Learning Rates')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùóÔ∏è <u>Warning</u> ‚ùóÔ∏è Too low and too high learning rates both lead to a high test loss... but not for the same reasons!\n",
    "\n",
    "* A **low learning rate** helps a neural network converge in a similar fashion to a moderate learning rate but... way slower... i.e. more epochs would be needed!\n",
    "* A **high learning rate** makes the algorithm diverge completely.\n",
    "    - Try a learning rate $ \\alpha = 10 $ with 400 epochs, you should see the loss vary. This corresponds to the fact that the algorithms converge to *different local minima*\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üèÅ Congratulations!\n",
    "\n",
    "üíæ Do not forget to `git add/commit/push` your notebook...\n",
    "\n",
    "üöÄ ... and move to the next challenge!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
